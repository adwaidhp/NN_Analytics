
# ğŸ§  Neural Network Project â€” ML S5 CSE

> A custom-built **Feedforward Neural Network in C++** designed to learn the linear function
> [
> y = 10x + 18
> ]
> using sigmoid and leaky ReLU activations.
> This project demonstrates the fundamentals of backpropagation, layer design, and gradient-based learning.

---

## âš™ï¸ Configuration & Setup

### ğŸ§© **Network Initialization**

```cpp
node_list = {
    {{ACT_FUN::sigmoid, 2.0}, {ACT_FUN::sigmoid, 3.0}, {ACT_FUN::sigmoid, 5.0}},
    {{ACT_FUN::leaky_relu, 4.0}, {ACT_FUN::leaky_relu, 4.0}, {ACT_FUN::leaky_relu, 4.0}},
    {{ACT_FUN::leaky_relu, 7.0}, {ACT_FUN::leaky_relu, 3.0}, {ACT_FUN::leaky_relu, 3.0}}
};

_nn.initialize(node_list);
```

---

## ğŸ§  Network Architecture

| **Layer** | **Type**     | **Neurons** | **Activation Function** |
| --------- | ------------ | ----------- | ----------------------- |
| Layer 0   | Input Layer  | 3           | `sigmoid`               |
| Layer 1   | Hidden Layer | 3           | `leaky_relu`            |
| Layer 2   | Output Layer | 3           | `leaky_relu`            |

---

## ğŸ“Š Dataset Details

The dataset follows the mapping **y = 10x + 18**, with 3 sets of inputs and expected outputs:

| **Input (xâ‚, xâ‚‚, xâ‚ƒ)** | **Expected Output (yâ‚, yâ‚‚, yâ‚ƒ)** |
| ---------------------- | -------------------------------- |
| (1, 2, 3)              | (28, 38, 48)                     |
| (0, 4, 5)              | (18, 58, 68)                     |
| (6, 7, 8)              | (78, 88, 98)                     |

```cpp
std::list<Dataset> datasets = {
    {{1, 2, 3}, {28, 38, 48}},
    {{0, 4, 5}, {18, 58, 68}},
    {{6, 7, 8}, {78, 88, 98}},
};
```

---

## ğŸ”§ Training Configuration

| **Parameter**       | **Value**                |
| ------------------- | ------------------------ |
| **Initial Weights** | 1.0                      |
| **Learning Rate**   | 0.001                    |
| **Error Threshold** | 0.001                    |
| **Epochs**          | 10                       |
| **Loss Function**   | Mean Squared Error (MSE) |

---

## ğŸš€ Execution Flow

1. **Initialize** the network with the given structure (`sigmoid â†’ leaky_relu â†’ leaky_relu`).
2. **Feedforward**: Pass input values through each layer.
3. **Compute Error**: Compare predicted and target values using MSE.
4. **Backpropagate**: Adjust weights using gradients.
5. **Repeat** for all epochs or until the error threshold is reached.

---

## ğŸ§¾ Sample Output

```
Input: [1.000000 2.000000 3.000000]
Label: [28.000000 38.000000 48.000000]
Output: [29.403426 39.956589 50.219187]
Error: [-1.473769 -2.057333 -2.321787]

Input: [0.000000 4.000000 5.000000]
Label: [18.000000 58.000000 68.000000]
Output: [18.610650 55.691571 65.831840]
Error: [-0.631367 2.216553 2.099029]

Input: [6.000000 7.000000 8.000000]
Label: [78.000000 88.000000 98.000000]
Output: [60.719402 83.468694 93.680455]
Error: [13.452148 4.297980 4.129153]
```

---

## ğŸ’¡ Observations

* ğŸ§® **Initial predictions** show moderate deviations, reducing with each iteration.
* âš¡ **Sigmoid** smooths input values and stabilizes gradients.
* ğŸ” **Leaky ReLU** avoids dead neurons and supports continuous gradient flow.
* ğŸ¯ **Small learning rate (0.001)** ensures steady, stable convergence.

---

## ğŸ§° Tools & Technologies

* **Language:** C++
* **Libraries:** STL (`list`, `cmath`)
* **Concepts Implemented:**

  * Feedforward Propagation
  * Backpropagation Algorithm
  * Custom Activation Functions
  * Mean Squared Error Calculation

---

## ğŸ‘©â€ğŸ’» Author

**Dharshana K S**
ğŸ“ Dept. of Computer Science & Engineering
ğŸ“ TCR23CS030 â€“ ML S5 CSE